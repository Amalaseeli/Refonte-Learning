{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76863ca-3f37-44d7-8c4e-cc92496e1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25cbffc7-a918-441a-9095-a4429d705cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca505bd-d0c1-4a24-81a4-e806dbf4d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=6\n",
    "head_size=1\n",
    "block_size=8\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    '''one head in self attention'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_emb,head_size)\n",
    "        self.query=nn.Linear(n_emb,head_size)\n",
    "        self.value=nn.Linear(n_emb,head_size)\n",
    "        \n",
    "        self.register_buffer('trill', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch, blocks, X = x.shape\n",
    "        key = self.key(x) # batch, block_size, X -- shape\n",
    "        query = self.query(x) # batch, block_size, X -- shape\n",
    "        weight = query @ key.transpose(-2, -1) * X ** (-0.5)\n",
    "        weight=weight.masked_fill(self.trill[:blocks, :blocks] ==0 , float('-inf'))\n",
    "        weight=F.softmax(weight, dim=-1)\n",
    "        out = weight @ self.value(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0645fe9-a715-4534-b8af-190e7b01a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Head(\n",
       "  (key): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (query): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (value): Linear(in_features=6, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=Head(2)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9251e6-cff9-4b95-a542-e536d8fb1c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]],\n",
       "\n",
       "        [[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]],\n",
       "\n",
       "        [[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(torch.zeros(3,8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e58e59-8d48-4032-9cb4-60c9d3ad6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''multihead in self attention'''\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super(),__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.layer=nn.Linear(n_emb,n_emb)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.head], dim=-1)\n",
    "        return self.layer(out)\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41111550-2075-4bc0-891d-d11073e8d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-08-23 13:22:11--  https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5583374 (5.3M) [text/plain]\n",
      "Saving to: 'C:/Users/User/Documents/amala/Refonte-Learning/shakepeare_s_plays.txt.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.62M 3s\n",
      "    50K .......... .......... .......... .......... ..........  1% 27.1M 2s\n",
      "   100K .......... .......... .......... .......... ..........  2% 5.54M 1s\n",
      "   150K .......... .......... .......... .......... ..........  3% 2.96M 2s\n",
      "   200K .......... .......... .......... .......... ..........  4% 3.72M 1s\n",
      "   250K .......... .......... .......... .......... ..........  5% 5.27M 1s\n",
      "   300K .......... .......... .......... .......... ..........  6% 3.00M 1s\n",
      "   350K .......... .......... .......... .......... ..........  7% 4.13M 1s\n",
      "   400K .......... .......... .......... .......... ..........  8% 2.42M 1s\n",
      "   450K .......... .......... .......... .......... ..........  9% 4.47M 1s\n",
      "   500K .......... .......... .......... .......... .......... 10% 5.70M 1s\n",
      "   550K .......... .......... .......... .......... .......... 11% 3.47M 1s\n",
      "   600K .......... .......... .......... .......... .......... 11% 5.23M 1s\n",
      "   650K .......... .......... .......... .......... .......... 12% 3.87M 1s\n",
      "   700K .......... .......... .......... .......... .......... 13% 6.33M 1s\n",
      "   750K .......... .......... .......... .......... .......... 14% 4.04M 1s\n",
      "   800K .......... .......... .......... .......... .......... 15% 7.17M 1s\n",
      "   850K .......... .......... .......... .......... .......... 16% 2.87M 1s\n",
      "   900K .......... .......... .......... .......... .......... 17% 6.89M 1s\n",
      "   950K .......... .......... .......... .......... .......... 18% 6.03M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 19% 4.99M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 20% 6.42M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 21% 6.53M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 22% 4.94M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 6.41M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 23% 6.05M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 6.65M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 4.37M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 7.14M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 27% 2.86M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 28% 5.58M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 29% 5.17M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 30% 6.77M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 31% 6.38M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 32% 5.58M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 33% 5.35M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 33% 4.58M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 6.09M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 35% 7.87M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 36% 3.76M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 37% 6.65M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 38% 7.12M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 39% 4.75M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 40% 5.91M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 41% 2.07M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 42% 6.73M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 43% 6.44M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 44% 5.70M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 44% 6.79M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 45% 6.31M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 46% 5.28M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 47% 1.88M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 48% 5.53M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 49% 6.57M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 50% 5.58M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 51% 5.74M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 52% 6.43M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 53% 4.75M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 54% 6.21M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 55% 5.23M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 55% 6.38M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 56% 5.48M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 57% 6.36M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 58% 5.02M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 59% 5.52M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 60% 6.89M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 61% 3.69M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 62% 5.32M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 63% 6.72M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 64% 4.61M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 65% 5.44M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 66% 5.19M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 66% 3.52M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 67% 6.28M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 68% 6.67M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 69% 1.67M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 70% 2.70M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 71% 13.9M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 72% 3.18M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 73% 16.4M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 74% 5.27M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 75% 5.27M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 76% 5.20M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 77% 5.35M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 77% 5.08M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 78% 6.30M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 79% 6.42M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 80% 5.97M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 81% 4.83M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 82% 5.52M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 83% 6.71M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 84% 3.98M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 85% 6.53M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 86% 5.83M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 87% 6.64M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 88% 5.86M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 88% 5.80M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 89% 5.81M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 90% 6.74M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 91% 5.23M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 92% 6.33M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 93% 5.77M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 94% 4.79M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 95% 5.60M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 96% 4.43M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 97% 7.26M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 98% 5.53M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 99% 5.60M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 99% 7.92M 0s\n",
      "  5450K ..                                                    100% 47.9G=1.1s\n",
      "\n",
      "2024-08-23 13:22:13 (4.93 MB/s) - 'C:/Users/User/Documents/amala/Refonte-Learning/shakepeare_s_plays.txt.1' saved [5583374/5583374]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P C:\\Users\\User\\Documents\\amala\\Refonte-Learning https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8246f692-73c4-4848-9057-ed948fd6b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakepeare_s_plays.txt', 'r', encoding=\"utf8\") as f:\n",
    "  text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cca611-c77b-45bf-b138-7e46e65ddb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Hamlet\n",
      "\n",
      "ACT I\n",
      "SCENE I. Elsinore. A platform before the castle.\n",
      "\n",
      "    FRANCISCO at his post. Enter t\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9aac9cb-e496-4e75-bbb3-f3250f7c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a:1, b:,2 ....z:26 A:27 etc\n",
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555da603-e612-41a6-a525-ae6714c2335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !#$&'(),-.0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzÀè—‘’“”… 90\n"
     ]
    }
   ],
   "source": [
    "print(''.join(chars),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06cbbbe6-0dc7-4ec2-a058-a6507f79f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63, 64]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding and decoding\n",
    "str_to_int={char:i for i,char in enumerate(chars)}\n",
    "int_to_str={i:char for i,char in enumerate(chars)}\n",
    "\n",
    "encode=lambda string:[str_to_int[s] for s in string]\n",
    "decode=lambda indexs:[int_to_str[i] for i in indexs]\n",
    "\n",
    "encode('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320a2dab-60b7-4ee0-af5f-d35c9037d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'i']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b32f2d-df39-4f03-adb5-95fc9e266b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(encode('hi')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc7d3e5-5a80-467c-bd4f-d2979bb1d19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi work!!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(encode('hi work!!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d537bc93-f190-4f8c-8903-dc1c133c30e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5580526"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d64a9d44-6716-4689-8b9e-a3fbae8b33d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  2, 35, 56, 68, 67, 60, 75,  1,  1, 28, 30, 47,  2, 36,  1, 46, 30,\n",
       "         32, 41, 32,  2, 36, 12,  2, 32, 67, 74, 64, 69, 70, 73, 60, 12,  2, 28,\n",
       "          2, 71, 67, 56, 75, 61, 70, 73, 68,  2, 57, 60, 61, 70, 73, 60,  2, 75,\n",
       "         63, 60,  2, 58, 56, 74, 75, 67, 60, 12,  1,  1,  2,  2,  2,  2, 33, 45,\n",
       "         28, 41, 30, 36, 46, 30, 42,  2, 56, 75,  2, 63, 64, 74,  2, 71, 70, 74,\n",
       "         75, 12,  2, 32, 69, 75, 60, 73,  2, 75]),\n",
       " torch.int64,\n",
       " torch.Size([5580526]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data=torch.tensor(encode(text), dtype=torch.int64)\n",
    "data[:100],data.dtype, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317660d6-e12e-4df8-a5ae-f38dabeef5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=int(0.9*len(data))\n",
    "train,test=data[:splitter],data[splitter:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a8c3cc8-9209-40e8-abbf-960db4131d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  2, 35, 56, 68, 67, 60, 75,  1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ced84e3-f324-413a-a0ad-3fa27c1180b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my context:tensor([4]), while the target is:2\n",
      "This is my context:tensor([4, 2]), while the target is:35\n",
      "This is my context:tensor([ 4,  2, 35]), while the target is:56\n",
      "This is my context:tensor([ 4,  2, 35, 56]), while the target is:68\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68]), while the target is:67\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67]), while the target is:60\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67, 60]), while the target is:75\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67, 60, 75]), while the target is:1\n"
     ]
    }
   ],
   "source": [
    "x=data[:block_size]\n",
    "y=data[1:block_size+1]\n",
    "\n",
    "for next in range(block_size):\n",
    "    context=x[:next+1]\n",
    "    target=y[next]\n",
    "    print(f\"This is my context:{context}, while the target is:{target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68385972-9698-486b-8bcf-0b5071a7229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5022473\n",
      "This is my context:tensor([60]), while the target is:67\n",
      "This is my context:tensor([60, 67]), while the target is:67\n",
      "This is my context:tensor([60, 67, 67]), while the target is:2\n",
      "This is my context:tensor([60, 67, 67,  2]), while the target is:56\n",
      "This is my context:tensor([60, 67, 67,  2, 56]), while the target is:74\n",
      "This is my context:tensor([60, 67, 67,  2, 56, 74]), while the target is:2\n",
      "This is my context:tensor([60, 67, 67,  2, 56, 74,  2]), while the target is:57\n",
      "This is my context:tensor([60, 67, 67,  2, 56, 74,  2, 57]), while the target is:80\n",
      "This is my context:tensor([80]), while the target is:2\n",
      "This is my context:tensor([80,  2]), while the target is:64\n",
      "This is my context:tensor([80,  2, 64]), while the target is:74\n",
      "This is my context:tensor([80,  2, 64, 74]), while the target is:2\n",
      "This is my context:tensor([80,  2, 64, 74,  2]), while the target is:75\n",
      "This is my context:tensor([80,  2, 64, 74,  2, 75]), while the target is:63\n",
      "This is my context:tensor([80,  2, 64, 74,  2, 75, 63]), while the target is:64\n",
      "This is my context:tensor([80,  2, 64, 74,  2, 75, 63, 64]), while the target is:74\n",
      "This is my context:tensor([63]), while the target is:80\n",
      "This is my context:tensor([63, 80]), while the target is:2\n",
      "This is my context:tensor([63, 80,  2]), while the target is:63\n",
      "This is my context:tensor([63, 80,  2, 63]), while the target is:56\n",
      "This is my context:tensor([63, 80,  2, 63, 56]), while the target is:73\n",
      "This is my context:tensor([63, 80,  2, 63, 56, 73]), while the target is:69\n",
      "This is my context:tensor([63, 80,  2, 63, 56, 73, 69]), while the target is:60\n",
      "This is my context:tensor([63, 80,  2, 63, 56, 73, 69, 60]), while the target is:74\n"
     ]
    }
   ],
   "source": [
    "batch_size=3\n",
    "def batches(split):\n",
    "   data=train if split=='train' else test\n",
    "   #randmly selecting\n",
    "   print(len(data))\n",
    "   indexes=torch.randint(len(data)-block_size, (batch_size,))\n",
    "   x=torch.stack([data[i:i+block_size] for i in indexes])\n",
    "   y=torch.stack([data[i+1:i+1+block_size] for i in indexes])\n",
    "   return x,y\n",
    "   #print(indexes)\n",
    "x,y=batches('train')\n",
    "#print(x.shape)\n",
    "#print(x)\n",
    "#print(y)\n",
    "for b in range(batch_size):\n",
    "    for next in range(block_size):\n",
    "        context=x[b,:next+1]\n",
    "        target=y[b,next]\n",
    "        print(f\"This is my context:{context}, while the target is:{target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd5d2632-1dfa-4603-b344-d8188498c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 1, 90])\n",
      "tensor([[-1.8129,  0.7621, -0.0365,  0.9324,  2.1147,  1.5161,  0.6990,  0.0609,\n",
      "          0.1383,  1.1420,  0.4576, -0.5154,  0.7747, -0.4448, -1.3667, -0.6333,\n",
      "          1.0655, -2.3393, -0.6099, -0.2064, -0.8126, -1.6596, -0.0900,  0.1723,\n",
      "         -1.5762,  0.0262, -0.2389,  1.0453,  1.6601, -0.3469,  0.0386,  0.0477,\n",
      "         -0.2516, -0.0183, -0.1743,  1.1065,  1.2372, -0.8451,  0.5743, -0.8301,\n",
      "         -0.4035, -0.6499, -0.3282,  1.3803,  0.0377,  1.0299, -0.1952,  0.1535,\n",
      "         -1.7138,  0.6067,  0.0058, -0.0295,  1.3418,  1.3028, -0.9031,  1.0262,\n",
      "          0.7465,  1.0692,  0.9010,  0.3545, -0.1480, -0.7613,  0.0462,  0.2363,\n",
      "         -0.5312, -0.1501, -0.1388, -0.2764, -1.2187, -0.7612, -0.1348, -0.3404,\n",
      "          0.9116, -0.9193,  0.4334, -0.0486,  2.7682,  1.1959,  0.3251, -0.7664,\n",
      "         -0.8045,  0.0730, -1.1163,  1.9472, -0.2097, -0.0771, -0.9505, -1.3920,\n",
      "          0.7582, -0.7046]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TextGeni(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lookup_token_emd_table=nn.Embedding(vocab_size,vocab_size)\n",
    "    def forward(self,x):\n",
    "        batches, block_size=x.shape\n",
    "        out=self.lookup_token_emd_table(x)\n",
    "        return out\n",
    "    def generate(self, x, max_tokens):\n",
    "        for _ in range(max_tokens):\n",
    "            logits=self(x)\n",
    "            #print(logits)\n",
    "            print(logits.shape)\n",
    "            logits=logits[:, -1, :]\n",
    "            print(logits)\n",
    "            probabilities=F.softmax(logits, dim=-1)\n",
    "            \n",
    "        \n",
    "geni=TextGeni() \n",
    "output=geni(x)\n",
    "#print(output.shape)\n",
    "#print(output)\n",
    "geni.generate((torch.zeros((1,1),dtype=torch.long)),10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b845772-b305-42f2-95f0-f95959a9c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P$$#KR]>3&èuz#AA3ku!#P=vX&Zz—A—W#vè1uHwrèPuDèzL3:)BIdèQYYSu\n",
      "2\n",
      "ov'u?CU?$RVPA?cb.?qb'=u?  ugY9H$$A#u$auuAè”HzbzsxZ]uu!è cè’P'è-)R#$$è=uA.Pl]))u#k$]…fg=H4bh#kH:ba#YG:$èèTcu3]Y29—pè”cGubE—#R!Av”u#l#utwM#P\n"
     ]
    }
   ],
   "source": [
    "geni_text = []\n",
    "def generate(x, max_tokens):\n",
    "    for _ in range(max_tokens):\n",
    "      logits = geni(x)\n",
    "      logit = logits[:, -1, :]\n",
    "      # print(logits.shape)\n",
    "      probilities = F.softmax(logits, dim=-1).view(-1, vocab_size) # 1, 90\n",
    "      next_x = torch.multinomial(probilities, num_samples=1)\n",
    "      geni_text.append(int(next_x))\n",
    "    return geni_text\n",
    "      # print(next_x)\n",
    "print(''.join(decode(generate(torch.zeros((1,1), dtype=torch.long), 200))))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39542111-bf2f-47cb-8e43-8bab8316133f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"P$$#KR]>3&èuz#AA3ku!#P=vX&Zz—A—W#vè1uHwrèPuDèzL3:)BIdèQYYSu\\n2\\nov'u?CU?$RVPA?cb.?qb'=u?  ugY9H$$A#u$auuAè”HzbzsxZ]uu!è cè’P'è-)R#$$è=uA.Pl]))u#k$]…fg=H4bh#kH:ba#YG:$èèTcu3]Y29—pè”cGubE—#R!Av”u#l#utwM#P\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(geni_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c064cd6d-f52e-4033-b235-6f960b51352b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b050d252-da2c-47ca-bf45-5b09de70de23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
