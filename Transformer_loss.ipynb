{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b76863ca-3f37-44d7-8c4e-cc92496e1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25cbffc7-a918-441a-9095-a4429d705cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca505bd-d0c1-4a24-81a4-e806dbf4d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_emb=6\n",
    "head_size=1\n",
    "block_size=8\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    '''one head in self attention'''\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_emb,head_size)\n",
    "        self.query=nn.Linear(n_emb,head_size)\n",
    "        self.value=nn.Linear(n_emb,head_size)\n",
    "        \n",
    "        self.register_buffer('trill', torch.tril(torch.ones(block_size,block_size)))\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch, blocks, X = x.shape\n",
    "        key = self.key(x) # batch, block_size, X -- shape\n",
    "        query = self.query(x) # batch, block_size, X -- shape\n",
    "        weight = query @ key.transpose(-2, -1) * X ** (-0.5)\n",
    "        weight=weight.masked_fill(self.trill[:blocks, :blocks] ==0 , float('-inf'))\n",
    "        weight=F.softmax(weight, dim=-1)\n",
    "        out = weight @ self.value(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0645fe9-a715-4534-b8af-190e7b01a9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Head(\n",
       "  (key): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (query): Linear(in_features=6, out_features=2, bias=True)\n",
       "  (value): Linear(in_features=6, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h=Head(2)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a9251e6-cff9-4b95-a542-e536d8fb1c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]],\n",
       "\n",
       "        [[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]],\n",
       "\n",
       "        [[-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735],\n",
       "         [-0.3627,  0.3735]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(torch.zeros(3,8,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52e58e59-8d48-4032-9cb4-60c9d3ad6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    '''multihead in self attention'''\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super(),__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.layer=nn.Linear(n_emb,n_emb)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.head], dim=-1)\n",
    "        return self.layer(out)\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41111550-2075-4bc0-891d-d11073e8d1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-08-23 13:22:11--  https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt\n",
      "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5583374 (5.3M) [text/plain]\n",
      "Saving to: 'C:/Users/User/Documents/amala/Refonte-Learning/shakepeare_s_plays.txt.1'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.62M 3s\n",
      "    50K .......... .......... .......... .......... ..........  1% 27.1M 2s\n",
      "   100K .......... .......... .......... .......... ..........  2% 5.54M 1s\n",
      "   150K .......... .......... .......... .......... ..........  3% 2.96M 2s\n",
      "   200K .......... .......... .......... .......... ..........  4% 3.72M 1s\n",
      "   250K .......... .......... .......... .......... ..........  5% 5.27M 1s\n",
      "   300K .......... .......... .......... .......... ..........  6% 3.00M 1s\n",
      "   350K .......... .......... .......... .......... ..........  7% 4.13M 1s\n",
      "   400K .......... .......... .......... .......... ..........  8% 2.42M 1s\n",
      "   450K .......... .......... .......... .......... ..........  9% 4.47M 1s\n",
      "   500K .......... .......... .......... .......... .......... 10% 5.70M 1s\n",
      "   550K .......... .......... .......... .......... .......... 11% 3.47M 1s\n",
      "   600K .......... .......... .......... .......... .......... 11% 5.23M 1s\n",
      "   650K .......... .......... .......... .......... .......... 12% 3.87M 1s\n",
      "   700K .......... .......... .......... .......... .......... 13% 6.33M 1s\n",
      "   750K .......... .......... .......... .......... .......... 14% 4.04M 1s\n",
      "   800K .......... .......... .......... .......... .......... 15% 7.17M 1s\n",
      "   850K .......... .......... .......... .......... .......... 16% 2.87M 1s\n",
      "   900K .......... .......... .......... .......... .......... 17% 6.89M 1s\n",
      "   950K .......... .......... .......... .......... .......... 18% 6.03M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 19% 4.99M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 20% 6.42M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 21% 6.53M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 22% 4.94M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 22% 6.41M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 23% 6.05M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 24% 6.65M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 25% 4.37M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 26% 7.14M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 27% 2.86M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 28% 5.58M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 29% 5.17M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 30% 6.77M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 31% 6.38M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 32% 5.58M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 33% 5.35M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 33% 4.58M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 34% 6.09M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 35% 7.87M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 36% 3.76M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 37% 6.65M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 38% 7.12M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 39% 4.75M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 40% 5.91M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 41% 2.07M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 42% 6.73M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 43% 6.44M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 44% 5.70M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 44% 6.79M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 45% 6.31M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 46% 5.28M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 47% 1.88M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 48% 5.53M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 49% 6.57M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 50% 5.58M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 51% 5.74M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 52% 6.43M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 53% 4.75M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 54% 6.21M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 55% 5.23M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 55% 6.38M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 56% 5.48M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 57% 6.36M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 58% 5.02M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 59% 5.52M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 60% 6.89M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 61% 3.69M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 62% 5.32M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 63% 6.72M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 64% 4.61M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 65% 5.44M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 66% 5.19M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 66% 3.52M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 67% 6.28M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 68% 6.67M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 69% 1.67M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 70% 2.70M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 71% 13.9M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 72% 3.18M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 73% 16.4M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 74% 5.27M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 75% 5.27M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 76% 5.20M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 77% 5.35M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 77% 5.08M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 78% 6.30M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 79% 6.42M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 80% 5.97M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 81% 4.83M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 82% 5.52M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 83% 6.71M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 84% 3.98M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 85% 6.53M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 86% 5.83M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 87% 6.64M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 88% 5.86M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 88% 5.80M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 89% 5.81M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 90% 6.74M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 91% 5.23M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 92% 6.33M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 93% 5.77M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 94% 4.79M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 95% 5.60M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 96% 4.43M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 97% 7.26M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 98% 5.53M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 99% 5.60M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 99% 7.92M 0s\n",
      "  5450K ..                                                    100% 47.9G=1.1s\n",
      "\n",
      "2024-08-23 13:22:13 (4.93 MB/s) - 'C:/Users/User/Documents/amala/Refonte-Learning/shakepeare_s_plays.txt.1' saved [5583374/5583374]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P C:\\Users\\User\\Documents\\amala\\Refonte-Learning https://gist.githubusercontent.com/Momnadar1/8805a6d53e92d6be17b9837711a5931a/raw/adc9cc97efc92232f01cbb6a1b13e8123d9f8f8d/shakepeare_s_plays.txt --no-check-certificate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8246f692-73c4-4848-9057-ed948fd6b4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakepeare_s_plays.txt', 'r', encoding=\"utf8\") as f:\n",
    "  text = f.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86cca611-c77b-45bf-b138-7e46e65ddb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Hamlet\n",
      "\n",
      "ACT I\n",
      "SCENE I. Elsinore. A platform before the castle.\n",
      "\n",
      "    FRANCISCO at his post. Enter t\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9aac9cb-e496-4e75-bbb3-f3250f7c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a:1, b:,2 ....z:26 A:27 etc\n",
    "chars=sorted(list(set(text)))\n",
    "vocab_size=len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "555da603-e612-41a6-a525-ae6714c2335a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !#$&'(),-.0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyzÀè—‘’“”… 90\n"
     ]
    }
   ],
   "source": [
    "print(''.join(chars),vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06cbbbe6-0dc7-4ec2-a058-a6507f79f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63, 64]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding and decoding\n",
    "str_to_int={char:i for i,char in enumerate(chars)}\n",
    "int_to_str={i:char for i,char in enumerate(chars)}\n",
    "\n",
    "encode=lambda string:[str_to_int[s] for s in string]\n",
    "decode=lambda indexs:[int_to_str[i] for i in indexs]\n",
    "\n",
    "encode('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "320a2dab-60b7-4ee0-af5f-d35c9037d2c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', 'i']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode('hi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20b32f2d-df39-4f03-adb5-95fc9e266b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(encode('hi')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc7d3e5-5a80-467c-bd4f-d2979bb1d19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi work!!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(decode(encode('hi work!!')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d537bc93-f190-4f8c-8903-dc1c133c30e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5580526"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d64a9d44-6716-4689-8b9e-a3fbae8b33d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  2, 35, 56, 68, 67, 60, 75,  1,  1, 28, 30, 47,  2, 36,  1, 46, 30,\n",
       "         32, 41, 32,  2, 36, 12,  2, 32, 67, 74, 64, 69, 70, 73, 60, 12,  2, 28,\n",
       "          2, 71, 67, 56, 75, 61, 70, 73, 68,  2, 57, 60, 61, 70, 73, 60,  2, 75,\n",
       "         63, 60,  2, 58, 56, 74, 75, 67, 60, 12,  1,  1,  2,  2,  2,  2, 33, 45,\n",
       "         28, 41, 30, 36, 46, 30, 42,  2, 56, 75,  2, 63, 64, 74,  2, 71, 70, 74,\n",
       "         75, 12,  2, 32, 69, 75, 60, 73,  2, 75]),\n",
       " torch.int64,\n",
       " torch.Size([5580526]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data=torch.tensor(encode(text), dtype=torch.int64)\n",
    "data[:100],data.dtype, data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "317660d6-e12e-4df8-a5ae-f38dabeef5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter=int(0.9*len(data))\n",
    "train,test=data[:splitter],data[splitter:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a8c3cc8-9209-40e8-abbf-960db4131d28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  2, 35, 56, 68, 67, 60, 75,  1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ced84e3-f324-413a-a0ad-3fa27c1180b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my context:tensor([4]), while the target is:2\n",
      "This is my context:tensor([4, 2]), while the target is:35\n",
      "This is my context:tensor([ 4,  2, 35]), while the target is:56\n",
      "This is my context:tensor([ 4,  2, 35, 56]), while the target is:68\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68]), while the target is:67\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67]), while the target is:60\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67, 60]), while the target is:75\n",
      "This is my context:tensor([ 4,  2, 35, 56, 68, 67, 60, 75]), while the target is:1\n"
     ]
    }
   ],
   "source": [
    "x=data[:block_size]\n",
    "y=data[1:block_size+1]\n",
    "\n",
    "for next in range(block_size):\n",
    "    context=x[:next+1]\n",
    "    target=y[next]\n",
    "    print(f\"This is my context:{context}, while the target is:{target}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68385972-9698-486b-8bcf-0b5071a7229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my context:tensor([1]), while the target is:1\n",
      "This is my context:tensor([1, 1]), while the target is:30\n",
      "This is my context:tensor([ 1,  1, 30]), while the target is:39\n",
      "This is my context:tensor([ 1,  1, 30, 39]), while the target is:28\n",
      "This is my context:tensor([ 1,  1, 30, 39, 28]), while the target is:48\n",
      "This is my context:tensor([ 1,  1, 30, 39, 28, 48]), while the target is:31\n",
      "This is my context:tensor([ 1,  1, 30, 39, 28, 48, 31]), while the target is:36\n",
      "This is my context:tensor([ 1,  1, 30, 39, 28, 48, 31, 36]), while the target is:42\n",
      "This is my context:tensor([60]), while the target is:2\n",
      "This is my context:tensor([60,  2]), while the target is:74\n",
      "This is my context:tensor([60,  2, 74]), while the target is:60\n",
      "This is my context:tensor([60,  2, 74, 60]), while the target is:60\n",
      "This is my context:tensor([60,  2, 74, 60, 60]), while the target is:69\n",
      "This is my context:tensor([60,  2, 74, 60, 60, 69]), while the target is:2\n",
      "This is my context:tensor([60,  2, 74, 60, 60, 69,  2]), while the target is:56\n",
      "This is my context:tensor([60,  2, 74, 60, 60, 69,  2, 56]), while the target is:69\n"
     ]
    }
   ],
   "source": [
    "batch_size=2\n",
    "def batches(split):\n",
    "   data=train if split=='train' else test\n",
    "   #randmly selecting\n",
    "   # print(len(data))\n",
    "   indexes=torch.randint(len(data)-block_size, (batch_size,))\n",
    "   x=torch.stack([data[i:i+block_size] for i in indexes])\n",
    "   y=torch.stack([data[i+1:i+1+block_size] for i in indexes])\n",
    "   return x,y\n",
    "   #print(indexes)\n",
    "x,y=batches('train')\n",
    "#print(x.shape)\n",
    "#print(x)\n",
    "#print(y)\n",
    "for b in range(batch_size):\n",
    "    for next in range(block_size):\n",
    "        context=x[b,:next+1]\n",
    "        target=y[b,next]\n",
    "        print(f\"This is my context:{context}, while the target is:{target}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd5d2632-1dfa-4603-b344-d8188498c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.972739219665527\n",
      "\tcSrNV7P[p=…alm“yy&,\n",
      "c4X2\n",
      "9LZpRNV=t[Z>#:E—\tG[TuÀd2qXMDTÀuq2&QcjÀ6”2!EH\tkE“Z\tè7Fkv6vEUINVPK$E(-;F7bWDi1HNCNPo=g3ÀtH'xv6qèVa0OSN=F5XVRhmk5\tAÀ7zDgz“D(10G85-'fI,MFbtq\n",
      "c9FI”CU“zBa(‘dae5Aje\n",
      "HqKDW=yrKO)Z“qpq4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "        #  x = [1, 25, 89, 65,63,64]\n",
    "          self.lookup_token_emd_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "      def forward(self, x, y=None): #\n",
    "          batches, block_size = x.shape\n",
    "          out = self.lookup_token_emd_table(x) # 2, 7, 90 , x: 1,2 3\n",
    "\n",
    "          if y is None:\n",
    "            loss = None\n",
    "          else:\n",
    "            batches, block_size, X = out.shape\n",
    "            loss = F.cross_entropy(out.view(batches*block_size, X), y.view(batches*block_size))\n",
    "\n",
    "          return out, loss\n",
    "\n",
    "      def generate(self, x, max_tokens=200):\n",
    "          for _ in range(max_tokens):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            # print(logits.shape)\n",
    "            probilities = F.softmax(logits, dim=-1) # 1, 90\n",
    "            next_x = torch.multinomial(probilities, num_samples=1)\n",
    "            x = torch.cat((x, next_x), dim=1) # [hi, ] 1 2 3\n",
    "          return x\n",
    "\n",
    "model = TextGenerator()\n",
    "out, loss = model(x, y)\n",
    "print(loss.item())\n",
    "\n",
    "predicted = model.generate(torch.zeros((1,1), dtype=torch.long))\n",
    "print(''.join(decode(predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7b845772-b305-42f2-95f0-f95959a9c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def losses_estimate():\n",
    "    output={}\n",
    "    model.eval()\n",
    "    for split in ['train','test']:\n",
    "        losses=torch.zeros(500)\n",
    "        for i in range(500):\n",
    "           x,y=batches(split)\n",
    "           logits, loss=model(x,y)\n",
    "           losses[i] = loss.item()\n",
    "        output[split]=losses.mean()\n",
    "    model.train()\n",
    "    return output\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7a289e3-bbe8-4f78-9cbe-d055b4ef7b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0081  M parameters\n"
     ]
    }
   ],
   "source": [
    "# numel return total numper of elements in input tensor\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6,' M parameters')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcd9a3e7-47c2-4e1f-9da1-6fa0b3e1e260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer=torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39542111-bf2f-47cb-8e43-8bab8316133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 0, training loss: 4.827  and val loss: 4.950\n",
      "Iteration no: 50, training loss: 4.893  and val loss: 4.964\n",
      "Iteration no: 100, training loss: 4.964  and val loss: 5.056\n",
      "Iteration no: 150, training loss: 4.973  and val loss: 4.882\n",
      "Iteration no: 200, training loss: 4.915  and val loss: 4.912\n",
      "Iteration no: 250, training loss: 4.882  and val loss: 4.931\n",
      "Iteration no: 300, training loss: 4.825  and val loss: 4.905\n",
      "Iteration no: 350, training loss: 4.875  and val loss: 4.936\n",
      "Iteration no: 400, training loss: 4.929  and val loss: 4.885\n",
      "Iteration no: 450, training loss: 4.808  and val loss: 4.845\n",
      "Iteration no: 500, training loss: 4.864  and val loss: 4.923\n",
      "Iteration no: 550, training loss: 4.867  and val loss: 5.018\n",
      "Iteration no: 600, training loss: 4.903  and val loss: 4.862\n",
      "Iteration no: 650, training loss: 4.944  and val loss: 4.970\n",
      "Iteration no: 700, training loss: 4.865  and val loss: 4.927\n",
      "Iteration no: 750, training loss: 4.878  and val loss: 4.944\n",
      "Iteration no: 800, training loss: 4.925  and val loss: 4.956\n",
      "Iteration no: 850, training loss: 4.864  and val loss: 4.931\n",
      "Iteration no: 900, training loss: 4.886  and val loss: 4.927\n",
      "Iteration no: 950, training loss: 4.844  and val loss: 4.949\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_iters = 1000\n",
    "for iter in range(max_iters):\n",
    "  if iter % 50 == 0 :\n",
    "    losses = losses_estimate()\n",
    "    print(f\"Iteration no: {iter}, training loss: {losses['train']:.3f}  and val loss: {losses['test']:.3f}\") \n",
    "\n",
    "    x,y=batches('train')\n",
    "    logits, loss=model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c064cd6d-f52e-4033-b235-6f960b51352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tDSPS-r'JI”8irWPwlONSQ9\n",
      "FVh\n",
      "è]F:1Y g#HT.fo.,T.Àk5“!$Ej-'fWz…;)'myb4;!uo’—Wfw’dèL7'x#3‘H'!mh‘$n2WjSt3x1jGaD?6S #gW#9W'']À\n",
      "i”:èGu1hV-OVvlK?elqXt3&pè‘ds.S‘9.a,G6 KZca].jkeC0n1mU=WSH0]5H9Lh‘1—F.C$4yBÀm\n",
      "“!!\n"
     ]
    }
   ],
   "source": [
    "predicted = model.generate(torch.zeros((1,1), dtype=torch.long))\n",
    "print(''.join(decode(predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b050d252-da2c-47ca-bf45-5b09de70de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "n_emb = 64\n",
    "block_size = 32\n",
    "head_size = 4\n",
    "# matrix\n",
    "\n",
    "# 64 rows : every char weight\n",
    "\n",
    "class Head(nn.Module):\n",
    "  \"\"\"\n",
    "  one head in self attention\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.key = nn.Linear(n_emb, head_size)\n",
    "    self.query = nn.Linear(n_emb, head_size)\n",
    "    self.value = nn.Linear(n_emb, head_size)\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    # tril: lower-triangular\n",
    "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, blocks, X = x.shape\n",
    "    query = self.query(x) # batch, block_size, X -- shape\n",
    "    key = self.key(x) # batch, block_size, X -- shape\n",
    "    weight = query @ key.transpose(-2, -1)  * X ** -0.5 # batch, block_size, X @ batch, X, blocl_size ---> batch, block_size, block_size\n",
    "    weight = weight.masked_fill(self.tril[:blocks, :blocks] == 0,float('-inf'))\n",
    "    weight = F.softmax(weight, dim=-1)\n",
    "    weight = self.dropout(weight)\n",
    "    out = weight @  self.value(x)\n",
    "    return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''multihead in self attention'''\n",
    "    def __init__(self, head_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.layer=nn.Linear(n_emb,n_emb)\n",
    "        self.dropout=nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=torch.cat([h(x) for h in self.head], dim=-1)\n",
    "        return self.dropout(self.layer(out))\n",
    "\n",
    "\n",
    "class FeedForwad(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.dff=nn.Sequential(\n",
    "            nn.Linear(n_emb,n_emb*4),\n",
    "            nn.Relu(),\n",
    "            nn.Linear(n_emb*4, n_emb),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.dff(x)\n",
    "        \n",
    "class BlockSeq(nn.Module):\n",
    "    def __init__(self, n_emb, num_heads):\n",
    "        super().__init__()\n",
    "        head_size=n_emb/num_heads\n",
    "        self.mh_att=MultiHeadAttention(head_size, num_heads)\n",
    "        self.ff_lay=FeedForwad(n_emb)\n",
    "        self.ln1=nn.LayerNorm(n_emb)\n",
    "        self.ln2=nn.LayerNorm(n_emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= x + self.mh_att(self.ln1(x))\n",
    "        x= x + self.ff_lay(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "n_x=2 #number of layers\n",
    "num_heads=4\n",
    "class TextGenerator(nn.Module):\n",
    "      def __init__(self):\n",
    "          super().__init__()\n",
    "        #  x = [1, 25, 89, 65,63,64]\n",
    "          self.lookup_token_emd_table = nn.Embedding(vocab_size, n_emb)\n",
    "          self.positional_encoding=nn.Embedding(block_size, n_emb)\n",
    "          self.blocks=nn.Sequential(*[BlockSeq(n_emb, num_heads) for _ in range(n_x)])\n",
    "          self.layer_norm=nn.LayerNorm(n_emb)\n",
    "          self.model_head=nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "      def forward(self, x, y=None): #\n",
    "          batches, block_size = x.shape\n",
    "          out = self.lookup_token_emd_table(x) # 2, 7, 90 , x: 1,2 3\n",
    "          pos_enc=self.positional_encoding(torch.arrange(block_size))\n",
    "          out= out + pos_enc\n",
    "          out=self.blocks(out)\n",
    "          out=self.layer_norm(out)\n",
    "          out=self.model_head(out)\n",
    "\n",
    "          if y is None:\n",
    "            loss = None\n",
    "          else:\n",
    "            batches, block_size, X = out.shape\n",
    "            loss = F.cross_entropy(out.view(batches*block_size, X), y.view(batches*block_size))\n",
    "\n",
    "          return out, loss\n",
    "\n",
    "      def generate(self, x, max_tokens=200):\n",
    "          for _ in range(max_tokens):\n",
    "            logits, _ = self(x)\n",
    "            logits = logits[:, -1, :]\n",
    "            # print(logits.shape)\n",
    "            probilities = F.softmax(logits, dim=-1) # 1, 90\n",
    "            next_x = torch.multinomial(probilities, num_samples=1)\n",
    "            x = torch.cat((x, next_x), dim=1) # [hi, ] 1 2 3\n",
    "          return x\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "842f5fbe-6c5c-4d84-bd60-1553e91aede8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration no: 0, training loss: 2.625  and val loss: 2.690\n",
      "Iteration no: 500, training loss: 2.631  and val loss: 2.704\n",
      "Iteration no: 1000, training loss: 2.626  and val loss: 2.707\n",
      "Iteration no: 1500, training loss: 2.626  and val loss: 2.693\n",
      "Iteration no: 2000, training loss: 2.638  and val loss: 2.702\n",
      "Iteration no: 2500, training loss: 2.629  and val loss: 2.708\n",
      "Iteration no: 3000, training loss: 2.635  and val loss: 2.699\n",
      "Iteration no: 3500, training loss: 2.617  and val loss: 2.703\n",
      "Iteration no: 4000, training loss: 2.629  and val loss: 2.719\n",
      "Iteration no: 4500, training loss: 2.615  and val loss: 2.726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "max_iters = 5000\n",
    "for iter in range(max_iters):\n",
    "  if iter % 500 == 0 :\n",
    "    losses = losses_estimate()\n",
    "    print(f\"Iteration no: {iter}, training loss: {losses['train']:.3f}  and val loss: {losses['test']:.3f}\") \n",
    "\n",
    "    x,y=batches('train')\n",
    "    logits, loss=model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e570d254-b39b-41df-9146-e11fb9f8d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t maroveE\n",
      "  Weejh icou\n",
      "  berks'sa  I  t  dEqFrm,  s t  or ERICZ,  dofthe, anzA I'rSp seEMExjqBACobeleve abqISennooX#H0jLAnt    y th n>zMkesu,Zhit2HAllllofoROundis  CERETz3ERt6prthythitQCO$PQDelow”?\n",
      "TEP\n"
     ]
    }
   ],
   "source": [
    "predicted = model.generate(torch.zeros((1,1), dtype=torch.long))\n",
    "print(''.join(decode(predicted[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa3184-a149-4024-b8da-d11c3dd66808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0a8cb-e215-4ded-8ba1-55bddaa4ca92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e91f01-3a75-4ca6-8f69-7e34b702ddba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
